{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Apache Spark com Delta Lake e Apache Iceberg","text":"<p>Este reposit\u00f3rio cont\u00e9m a implementa\u00e7\u00e3o de um projeto de Engenharia de Dados utilizando Apache Spark, Delta Lake e Apache Iceberg. O objetivo \u00e9 demonstrar opera\u00e7\u00f5es de manipula\u00e7\u00e3o de dados em ambientes Delta e Iceberg.</p>"},{"location":"#integrantes","title":"Integrantes","text":"<ul> <li>Anna Clara Teixeira de Medeiros</li> <li>Gabriel Antonin Pascoali</li> <li>Vinicius Teixeira Colombo</li> </ul>"},{"location":"#tecnologias-utilizadas","title":"Tecnologias Utilizadas","text":"<ul> <li>Apache Spark: Framework de processamento de dados distribu\u00eddos.</li> <li>Delta Lake: Camada de armazenamento transacional para Apache Spark.</li> <li>Apache Iceberg: Sistema de gerenciamento de tabelas de dados para grandes volumes de dados.</li> <li>PySpark: Interface Python para Apache Spark.</li> <li>Jupyter Labs: Ambiente de notebooks interativo para an\u00e1lise de dados.</li> </ul>"},{"location":"#objetivo","title":"Objetivo","text":"<p>Demonstrar como configurar e operar o Delta Lake e o Apache Iceberg no Apache Spark, utilizando opera\u00e7\u00f5es como <code>INSERT</code>, <code>UPDATE</code> e <code>DELETE</code> em tabelas.</p>"},{"location":"delta_lake/","title":"Delta Lake","text":""},{"location":"delta_lake/#o-que-e-o-delta-lake","title":"O que \u00e9 o Delta Lake?","text":"<p>O Delta Lake \u00e9 uma camada de armazenamento transacional para o Apache Spark que permite usar opera\u00e7\u00f5es ACID (Atomicidade, Consist\u00eancia, Isolamento e Durabilidade) em cima de arquivos de dados. Ele melhora o gerenciamento de dados, lidando com problemas como corrup\u00e7\u00e3o de dados, leitura consistente, e melhora o desempenho em grandes volumes de dados.</p>"},{"location":"delta_lake/#configuracao-do-delta-lake-no-apache-spark","title":"Configura\u00e7\u00e3o do Delta Lake no Apache Spark","text":"<p>Para configurar o Delta Lake no Apache Spark, voc\u00ea pode criar uma SparkSession com as configura\u00e7\u00f5es espec\u00edficas do Delta, como mostrado abaixo:</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, FloatType\n\nfrom delta import *\n\nspark = (\n    SparkSession\n    .builder\n    .master(\"local[*]\")\n    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.3.0\")\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n    .getOrCreate()\n)\n</code></pre> <p>Essa configura\u00e7\u00e3o permite usar o Delta Lake e suas funcionalidades em um ambiente PySpark local.</p>"},{"location":"delta_lake/#criando-tabelas-delta","title":"Criando Tabelas Delta","text":"<p>Com a configura\u00e7\u00e3o do SparkSession pronta, podemos criar uma tabela Delta. Aqui est\u00e1 um exemplo de como criar uma tabela chamada <code>location_delta</code> com informa\u00e7\u00f5es sobre localidades:</p> <pre><code>spark.sql(\"\"\"\n    CREATE TABLE location_delta (location_id INT, country STRING, continent STRING, population DOUBLE) \n    USING delta\n\"\"\")\n</code></pre>"},{"location":"delta_lake/#inserindo-dados-na-tabela-delta","title":"Inserindo Dados na Tabela Delta","text":"<p>Agora que a tabela foi criada, voc\u00ea pode inserir dados nela. Um exemplo de inser\u00e7\u00e3o seria:</p> <pre><code>spark.sql(\"\"\"\n    INSERT INTO location_delta \n    VALUES (1, 'Afghanistan', 'Asia', 41450000)\n\"\"\")\n</code></pre>"},{"location":"delta_lake/#consultando-dados-em-delta","title":"Consultando Dados em Delta","text":"<p>Voc\u00ea pode consultar os dados de uma tabela Delta como faria com qualquer outra tabela no Spark. Aqui est\u00e1 um exemplo para visualizar os dados da tabela <code>location_delta</code>:</p> <pre><code>spark.sql(\"SELECT * FROM location_delta\").show()\n</code></pre>"},{"location":"delta_lake/#exibindo-o-historico-de-operacoes","title":"Exibindo o Hist\u00f3rico de Opera\u00e7\u00f5es","text":"<p>Uma das caracter\u00edsticas poderosas do Delta Lake \u00e9 a capacidade de registrar e acessar o hist\u00f3rico de opera\u00e7\u00f5es em uma tabela. Isso \u00e9 feito atrav\u00e9s do comando <code>history()</code>, que nos mostra as opera\u00e7\u00f5es feitas sobre a tabela, como cria\u00e7\u00e3o, atualiza\u00e7\u00e3o e exclus\u00e3o de dados.</p> <pre><code>from delta.tables import DeltaTable\n\nlocation = DeltaTable.forPath(spark, \"./spark-warehouse/location_delta\")\nlocation.history().show()\n</code></pre>"},{"location":"delta_lake/#atualizando-dados","title":"Atualizando Dados","text":"<p>O Delta Lake tamb\u00e9m permite atualizar os dados de maneira eficiente. Por exemplo, se quisermos atualizar a popula\u00e7\u00e3o de um determinado pa\u00eds, podemos executar a seguinte opera\u00e7\u00e3o:</p> <pre><code>spark.sql(\"\"\"\n    UPDATE location_delta \n    SET population = 43000000 \n    WHERE location_id = 1\n\"\"\")\n</code></pre>"},{"location":"delta_lake/#deletando-dados","title":"Deletando Dados","text":"<p>Deletar dados tamb\u00e9m \u00e9 simples com Delta Lake. Para excluir um registro espec\u00edfico, como o local com <code>location_id</code> igual a 1, use o seguinte c\u00f3digo:</p> <pre><code>spark.sql(\"\"\"\n    DELETE FROM location_delta \n    WHERE location_id = 1\n\"\"\")\n</code></pre>"},{"location":"delta_lake/#adicionando-colunas","title":"Adicionando Colunas","text":"<p>Voc\u00ea pode adicionar novas colunas a uma tabela Delta usando o comando <code>ALTER TABLE</code>:</p> <pre><code>spark.sql(\"\"\"\n    ALTER TABLE location_delta ADD COLUMN people_vaccinated DOUBLE\n\"\"\")\n</code></pre>"},{"location":"delta_lake/#visualizando-dados-apos-alteracoes","title":"Visualizando Dados Ap\u00f3s Altera\u00e7\u00f5es","text":"<p>Ap\u00f3s a inser\u00e7\u00e3o de uma nova coluna, podemos visualizar os dados atualizados:</p> <pre><code>spark.sql(\"\"\"\n    SELECT * FROM location_delta\n\"\"\").show()\n</code></pre>"},{"location":"delta_lake/#historico-detalhado-das-operacoes","title":"Hist\u00f3rico Detalhado das Opera\u00e7\u00f5es","text":"<p>Voc\u00ea pode ver o hist\u00f3rico completo de todas as opera\u00e7\u00f5es realizadas na tabela Delta, incluindo os detalhes sobre o tipo de opera\u00e7\u00e3o e os par\u00e2metros envolvidos:</p> <pre><code>location.history().show(truncate=False)\n</code></pre>"},{"location":"delta_lake/#conclusao","title":"Conclus\u00e3o","text":"<p>Delta Lake permite um gerenciamento eficiente e seguro dos dados em um ambiente distribu\u00eddo como o Apache Spark. Com as opera\u00e7\u00f5es ACID e a capacidade de rastrear o hist\u00f3rico das tabelas, ele \u00e9 uma excelente escolha para sistemas de dados de larga escala.</p> <p>Esses exemplos demonstram as opera\u00e7\u00f5es b\u00e1sicas de manipula\u00e7\u00e3o de dados com Delta Lake e como utiliz\u00e1-lo para garantir consist\u00eancia, desempenho e integridade de dados.</p>"},{"location":"exemplos/","title":"Exemplos de C\u00f3digo","text":""},{"location":"exemplos/#exemplo-1-criando-uma-tabela-delta","title":"Exemplo 1: Criando uma Tabela Delta","text":"<p>Para criar uma tabela Delta, podemos usar o comando SQL. O exemplo abaixo cria uma tabela chamada <code>location_delta</code>, com colunas para <code>location_id</code>, <code>country</code>, <code>continent</code>, e <code>population</code>.</p> <pre><code>spark.sql(\"\"\"\n    CREATE TABLE location_delta (location_id INT, country STRING, continent STRING, population DOUBLE) \n    USING delta\n\"\"\")\n</code></pre>"},{"location":"exemplos/#exemplo-2-inserindo-dados-na-tabela-delta","title":"Exemplo 2: Inserindo Dados na Tabela Delta","text":"<p>Uma vez que a tabela foi criada, podemos inserir dados nela. O exemplo abaixo insere informa\u00e7\u00f5es sobre localidades na tabela <code>location_delta</code>:</p> <pre><code>spark.sql(\"\"\"\n    INSERT INTO location_delta \n    VALUES (1, 'Afghanistan', 'Asia', 41450000)\n\"\"\")\n</code></pre>"},{"location":"exemplos/#exemplo-3-atualizando-dados-em-tabela-delta","title":"Exemplo 3: Atualizando Dados em Tabela Delta","text":"<p>Para atualizar dados em uma tabela Delta, podemos usar uma consulta <code>UPDATE</code>. O exemplo abaixo altera o nome de um pa\u00eds na tabela <code>location_delta</code>.</p> <pre><code>spark.sql(\"\"\"\n    UPDATE delta.`/path/to/delta_table`\n    SET name = 'Jane'\n    WHERE id = 1\n\"\"\")\n</code></pre>"},{"location":"exemplos/#exemplo-4-deletando-dados-em-tabela-delta","title":"Exemplo 4: Deletando Dados em Tabela Delta","text":"<p>O comando <code>DELETE</code> pode ser usado para excluir dados da tabela. O exemplo abaixo exclui um registro espec\u00edfico da tabela <code>location_delta</code>.</p> <pre><code>spark.sql(\"\"\"\n    DELETE FROM delta.`/path/to/delta_table` WHERE id = 2\n\"\"\")\n</code></pre>"},{"location":"exemplos/#exemplo-5-criando-uma-tabela-iceberg","title":"Exemplo 5: Criando uma Tabela Iceberg","text":"<p>A cria\u00e7\u00e3o de tabelas no Iceberg tamb\u00e9m \u00e9 realizada atrav\u00e9s de SQL. O exemplo abaixo cria uma tabela chamada <code>covid_daily_vaccinations</code>, que armazena dados sobre vacina\u00e7\u00e3o di\u00e1ria.</p> <pre><code>spark.sql(\"\"\"\n    CREATE TABLE IF NOT EXISTS local.iceberg.covid_daily_vaccinations (\n        date DATE,\n        location STRING,\n        daily_vaccinations DOUBLE,\n        people_fully_vaccinated DOUBLE\n    ) USING iceberg PARTITIONED BY (year(date));\n\"\"\")\n</code></pre>"},{"location":"exemplos/#exemplo-6-inserindo-dados-na-tabela-iceberg","title":"Exemplo 6: Inserindo Dados na Tabela Iceberg","text":"<p>Depois de criar a tabela, podemos inserir dados nela. O exemplo abaixo insere informa\u00e7\u00f5es sobre vacina\u00e7\u00e3o di\u00e1ria para diferentes pa\u00edses.</p> <pre><code>spark.sql(\"\"\"\n    INSERT INTO local.iceberg.covid_daily_vaccinations\n    VALUES\n        (DATE('2025-04-21'), 'Brazil', 20000.0, 15000000.0),\n        (DATE('2025-04-20'), 'USA', 30000.0, 25000000.0),\n        (DATE('2025-04-19'), 'India', 40000.0, 35000000.0)\n\"\"\")\n</code></pre>"},{"location":"exemplos/#exemplo-7-atualizando-dados-em-tabela-iceberg","title":"Exemplo 7: Atualizando Dados em Tabela Iceberg","text":"<p>Da mesma forma que com Delta, tamb\u00e9m podemos atualizar dados em uma tabela Iceberg. O exemplo abaixo atualiza a quantidade de vacinas di\u00e1rias para o Brasil.</p> <pre><code>spark.sql(\"\"\"\n    UPDATE local.iceberg.covid_daily_vaccinations\n    SET daily_vaccinations = 50000.0\n    WHERE location = 'Brazil'\n    AND date = DATE('2025-04-21')\n\"\"\")\n</code></pre>"},{"location":"exemplos/#exemplo-8-deletando-dados-em-tabela-iceberg","title":"Exemplo 8: Deletando Dados em Tabela Iceberg","text":"<p>Se for necess\u00e1rio excluir um registro da tabela, podemos usar o comando <code>DELETE</code>. O exemplo abaixo deleta a linha de vacina\u00e7\u00e3o para o Brasil em 21 de abril de 2025.</p> <pre><code>spark.sql(\"\"\"\n    DELETE FROM local.iceberg.covid_daily_vaccinations\n    WHERE location = 'Brazil' AND date = DATE('2025-04-21')\n\"\"\")\n</code></pre>"},{"location":"exemplos/#exemplo-9-carregando-dados-de-arquivo-csv-para-iceberg","title":"Exemplo 9: Carregando Dados de Arquivo CSV para Iceberg","text":"<p>Voc\u00ea pode carregar dados externos, como um arquivo CSV, para uma tabela Iceberg, convertendo os tipos de dados conforme necess\u00e1rio. O exemplo abaixo l\u00ea um arquivo CSV de vacina\u00e7\u00e3o e carrega os dados na tabela <code>covid_daily_vaccinations</code>.</p> <pre><code>from pyspark.sql.functions import to_date, col\n\ndf = spark.read.csv(\"data/covid-19/vaccinations.csv\", header=True)\n\ndf = df.withColumn(\"date\", to_date(df[\"date\"], \"yyyy-MM-dd\"))\ndf = df.withColumn(\"daily_vaccinations\", col(\"daily_vaccinations\").cast(\"double\"))\ndf = df.withColumn(\"people_fully_vaccinated\", col(\"people_fully_vaccinated\").cast(\"double\"))\n\ndf.writeTo(\"local.iceberg.covid_daily_vaccinations\").append()\n</code></pre>"},{"location":"exemplos/#exemplo-10-consultando-dados-de-tabelas-iceberg","title":"Exemplo 10: Consultando Dados de Tabelas Iceberg","text":"<p>Aqui est\u00e1 como consultar os dados de uma tabela Iceberg. O exemplo abaixo mostra as vacinas di\u00e1rias para o Brasil em 21 de abril de 2025:</p> <pre><code>spark.sql(\"\"\"\n    SELECT * FROM local.iceberg.covid_daily_vaccinations \n    WHERE location = 'Brazil' AND date = DATE('2025-04-21')\n\"\"\").show()\n</code></pre>"},{"location":"exemplos/#conclusao","title":"Conclus\u00e3o","text":"<p>Esses exemplos ilustram as opera\u00e7\u00f5es b\u00e1sicas de manipula\u00e7\u00e3o de dados tanto no Delta Lake quanto no Apache Iceberg, desde a cria\u00e7\u00e3o de tabelas at\u00e9 opera\u00e7\u00f5es de leitura, escrita, atualiza\u00e7\u00e3o e exclus\u00e3o. O uso do Spark com essas camadas de armazenamento transacional garante que voc\u00ea possa manipular grandes volumes de dados de maneira eficiente e com integridade.</p>"},{"location":"iceberg/","title":"Apache Iceberg","text":""},{"location":"iceberg/#o-que-e-o-apache-iceberg","title":"O que \u00e9 o Apache Iceberg?","text":"<p>Apache Iceberg \u00e9 uma camada de abstra\u00e7\u00e3o para gerenciamento de tabelas de dados. Ele \u00e9 usado principalmente em ambientes de grandes volumes de dados e proporciona melhor gerenciamento de vers\u00f5es e efici\u00eancia em opera\u00e7\u00f5es como leitura, escrita, e atualiza\u00e7\u00e3o de dados.</p>"},{"location":"iceberg/#configuracao-do-apache-iceberg-no-apache-spark","title":"Configura\u00e7\u00e3o do Apache Iceberg no Apache Spark","text":"<p>Para configurar o Iceberg no Apache Spark, voc\u00ea precisa criar uma SparkSession com as configura\u00e7\u00f5es necess\u00e1rias, como mostrado abaixo. Esse c\u00f3digo inicializa o ambiente local para o desenvolvimento com Iceberg:</p> <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n  .appName(\"IcebergLocalDevelopment\") \\\n  .config(\"spark.driver.host\", \"localhost\") \\\n  .config('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.1') \\\n  .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n  .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n  .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n  .config(\"spark.sql.catalog.local.warehouse\", \"spark-warehouse/iceberg\") \\\n  .config(\"spark.sql.catalog.local.create-namespace\", \"true\") \\\n  .config(\"spark.driver.bindAddress\", \"localhost\") \\\n  .getOrCreate()\n</code></pre> <p>Esse c\u00f3digo garante que voc\u00ea tenha configurado corretamente a sess\u00e3o do Spark com Iceberg, incluindo a depend\u00eancia do Iceberg e o cat\u00e1logo de dados necess\u00e1rio.</p>"},{"location":"iceberg/#criando-tabelas-iceberg","title":"Criando Tabelas Iceberg","text":"<p>Aqui est\u00e1 o c\u00f3digo para criar uma tabela com dados de vacina\u00e7\u00e3o di\u00e1rios em diferentes pa\u00edses:</p> <pre><code>spark.sql(\"\"\"\n    CREATE TABLE IF NOT EXISTS local.iceberg.covid_daily_vaccinations (\n        date DATE,\n        location STRING,\n        daily_vaccinations DOUBLE,\n        people_fully_vaccinated DOUBLE\n    ) USING iceberg PARTITIONED BY (year(date));\n\"\"\")\n</code></pre> <p>Essa tabela \u00e9 particionada pelo ano da data, o que facilita consultas r\u00e1pidas com base em per\u00edodos.</p>"},{"location":"iceberg/#inserindo-dados-na-tabela-iceberg","title":"Inserindo Dados na Tabela Iceberg","text":"<p>Ap\u00f3s criar a tabela, podemos inserir dados nela. Aqui est\u00e1 um exemplo de inser\u00e7\u00e3o de dados sobre as vacinas di\u00e1rias para diferentes locais:</p> <pre><code>spark.sql(\"\"\"\n    INSERT INTO local.iceberg.covid_daily_vaccinations\n    VALUES\n        (DATE('2025-04-21'), 'Brazil', 20000.0, 15000000.0),\n        (DATE('2025-04-20'), 'USA', 30000.0, 25000000.0),\n        (DATE('2025-04-19'), 'India', 40000.0, 35000000.0)\n\"\"\")\n</code></pre> <p>Isso insere os dados diretamente na tabela Iceberg.</p>"},{"location":"iceberg/#atualizando-dados-na-tabela-iceberg","title":"Atualizando Dados na Tabela Iceberg","text":"<p>Em vez de sobrescrever dados antigos, voc\u00ea pode realizar atualiza\u00e7\u00f5es diretamente na tabela. O exemplo abaixo mostra como atualizar os dados de vacina\u00e7\u00e3o di\u00e1ria para o Brasil:</p> <pre><code>spark.sql(\"\"\"\n    UPDATE local.iceberg.covid_daily_vaccinations\n    SET daily_vaccinations = 50000.0\n    WHERE location = 'Brazil'\n    AND date = DATE('2025-04-21')\n\"\"\")\n</code></pre> <p>Isso ajusta a quantidade de vacinas administradas no Brasil na data especificada.</p>"},{"location":"iceberg/#consultando-dados-em-iceberg","title":"Consultando Dados em Iceberg","text":"<p>Para visualizar os dados ap\u00f3s a inser\u00e7\u00e3o ou atualiza\u00e7\u00e3o, utilize o comando <code>SELECT</code>. Aqui est\u00e1 um exemplo de consulta para mostrar os dados do Brasil em 21 de abril de 2025:</p> <pre><code>spark.sql(\"\"\"\n    SELECT * FROM local.iceberg.covid_daily_vaccinations \n    WHERE location = 'Brazil' AND date = DATE('2025-04-21')\n\"\"\").show()\n</code></pre> <p>Isso exibe os dados atualizados de vacina\u00e7\u00e3o para o Brasil.</p>"},{"location":"iceberg/#deletando-dados-em-iceberg","title":"Deletando Dados em Iceberg","text":"<p>O Iceberg tamb\u00e9m permite excluir dados de maneira eficiente. O exemplo abaixo mostra como deletar os dados de vacina\u00e7\u00e3o para o Brasil em 21 de abril de 2025:</p> <pre><code>spark.sql(\"\"\"\n    DELETE FROM local.iceberg.covid_daily_vaccinations\n    WHERE location = 'Brazil' AND date = DATE('2025-04-21')\n\"\"\")\n</code></pre> <p>Este comando remove a linha espec\u00edfica da tabela.</p>"},{"location":"iceberg/#carregando-e-manipulando-dados-externos","title":"Carregando e Manipulando Dados Externos","text":"<p>Aqui est\u00e1 um exemplo de como carregar dados de um arquivo CSV externo, ajustar tipos de dados e adicionar \u00e0 tabela Iceberg:</p> <pre><code>from pyspark.sql.functions import to_date, col\n\ndf = spark.read.csv(\"data/covid-19/vaccinations.csv\", header=True)\n\ndf = df.withColumn(\"date\", to_date(df[\"date\"], \"yyyy-MM-dd\"))\ndf = df.withColumn(\"daily_vaccinations\", col(\"daily_vaccinations\").cast(\"double\"))\ndf = df.withColumn(\"people_fully_vaccinated\", col(\"people_fully_vaccinated\").cast(\"double\"))\n\ndf.writeTo(\"local.iceberg.covid_daily_vaccinations\").append()\n</code></pre> <p>Este c\u00f3digo converte as colunas para o tipo adequado e insere os dados na tabela Iceberg.</p>"},{"location":"iceberg/#conclusao","title":"Conclus\u00e3o","text":"<p>O Apache Iceberg \u00e9 uma excelente solu\u00e7\u00e3o para trabalhar com grandes volumes de dados e realizar opera\u00e7\u00f5es como leitura, escrita, atualiza\u00e7\u00e3o e exclus\u00e3o de dados de maneira eficiente. Ele \u00e9 uma escolha ideal quando se trabalha com Spark e grandes conjuntos de dados em ambientes de produ\u00e7\u00e3o.</p>"},{"location":"instalacao/","title":"Instala\u00e7\u00e3o do Ambiente","text":"<p>Para configurar o ambiente necess\u00e1rio para este projeto, siga os passos abaixo:</p>"},{"location":"instalacao/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<ul> <li>Python 3.13</li> <li>UV (gerenciador de pacotes)</li> </ul>"},{"location":"instalacao/#passo-a-passo","title":"Passo a Passo","text":"<ol> <li>Clone o reposit\u00f3rio:</li> </ol> <p><code>bash    git clone https://github.com/lugialo/eg-dados-apache-spark.git    cd eg-dados-apache-spark</code></p> <ol> <li>Instale o UV:</li> </ol> <p><code>bash    curl -LsSf https://astral.sh/uv/install.sh | sh</code></p> <p>Ou, se preferir instalar via pip:</p> <p><code>bash    pip install uv</code></p> <ol> <li>Inicialize o projeto com o UV:</li> </ol> <p><code>bash    uv init</code></p> <ol> <li>Adicione as depend\u00eancias necess\u00e1rias:</li> </ol> <p><code>bash    uv add pyspark delta-spark iceberg-spark</code></p> <ol> <li>Crie e ative o ambiente virtual:</li> </ol> <p><code>bash    uv venv    source .venv/bin/activate  # No Windows: .venv\\Scripts\\activate</code></p> <ol> <li>Inicie o Jupyter Labs:</li> </ol> <p><code>bash    jupyter-lab</code></p> <p>Agora, voc\u00ea pode executar os notebooks <code>delta-lake.ipynb</code> e <code>pyspark-iceberg.ipynb</code> para testar as implementa\u00e7\u00f5es.</p>"},{"location":"referencias/","title":"Refer\u00eancias","text":"<ul> <li>Documenta\u00e7\u00e3o do Apache Spark</li> <li>Documenta\u00e7\u00e3o do Delta Lake</li> <li>Documenta\u00e7\u00e3o do Apache Iceberg</li> <li>Reposit\u00f3rio GitHub de Delta Lake</li> <li>Reposit\u00f3rio GitHub de Iceberg</li> <li>Reposit\u00f3rio Github CSV</li> </ul>"},{"location":"uso/","title":"Uso do Ambiente","text":"<p>Ap\u00f3s configurar o ambiente conforme descrito em Instala\u00e7\u00e3o do Ambiente, siga os passos abaixo para utilizar os notebooks:</p> <ol> <li>Abra o Jupyter Labs:</li> </ol> <p><code>bash    jupyter-lab</code></p> <ol> <li> <p>Navegue at\u00e9 o diret\u00f3rio do projeto e abra os notebooks:</p> </li> <li> <p><code>delta-lake.ipynb</code>: Demonstra opera\u00e7\u00f5es com Delta Lake.</p> </li> <li> <p><code>pyspark-iceberg.ipynb</code>: Demonstra opera\u00e7\u00f5es com Apache Iceberg.</p> </li> <li> <p>Execute as c\u00e9lulas dos notebooks para visualizar os resultados das opera\u00e7\u00f5es <code>INSERT</code>, <code>UPDATE</code> e <code>DELETE</code> em tabelas Delta e Iceberg.</p> </li> </ol>"}]}